# ═══════════════════════════════════════════════════════════════════════════
# Discord Notetaker Environment Configuration
# ═══════════════════════════════════════════════════════════════════════════
# Copy this file to .env and fill in your actual values.
# DO NOT commit .env to version control (it's in .gitignore)

# ── Discord Bot Configuration ────────────────────────────────────────────────
# Get your bot token from: https://discord.com/developers/applications
DISCORD_BOT_TOKEN=your_bot_token_here

# Your Discord server (guild) ID - Enable Developer Mode in Discord,
# right-click your server icon, and select "Copy Server ID"
DISCORD_GUILD_ID=123456789012345678

# Channel ID where the bot will post session summaries
# Right-click the channel and select "Copy Channel ID"
DISCORD_POST_CHANNEL_ID=234567890123456789

# ── AI Service Configuration ─────────────────────────────────────────────────
# Ollama LLM API endpoint
# Use http://127.0.0.1:11434 when bot is in host network mode
# Use http://ollama:11434 when bot is in bridge network mode
OLLAMA_HOST=http://127.0.0.1:11434

# LLM model to use for summarization
# Options: llama3.1:8b, llama3.1:13b, llama3.1:70b, mistral, etc.
# Run 'ollama list' to see available models
# Pull new models with: ollama pull llama3.1:8b
LLM_MODEL=llama3.1:8b

# Transcriber API endpoint
# Use http://127.0.0.1:8000 when bot is in host network mode
# Use http://transcriber:8000 when bot is in bridge network mode
TRANSCRIBER_URL=http://127.0.0.1:8000

# ── WhisperX Transcription Settings ──────────────────────────────────────────
# Compute device for WhisperX: cpu, cuda, or auto
# 'auto' will use CUDA if available, otherwise CPU
# CPU works but is slower; GPU (CUDA) is recommended for faster transcription
WHISPERX_DEVICE=cpu

# WhisperX model size: tiny, base, small, medium, large-v2, large-v3
# Larger models are more accurate but slower and require more memory
# Recommended: small for CPU, large-v2 for GPU
WHISPERX_MODEL=large-v2

# ── Data Storage ─────────────────────────────────────────────────────────────
# Path where session audio and transcripts are stored
# This is the path INSIDE the container
DATA_PATH=/app/data/sessions

# ── Optional: Debug Settings ─────────────────────────────────────────────────
# Enable verbose Discord voice event logging (0 or 1)
# VOICE_DEBUG=1

# Bot logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# BOT_LOG_LEVEL=INFO

# Register extra debug slash commands (/stun_check, /voice_endpoint, /intents)
# Set to 1 to expose these commands; default is 0 (hidden)
# EXPOSE_DEBUG_COMMANDS=0

# ─────────────────────────────────────────────────────────────────────────────
# SETUP CHECKLIST:
# 1. Create your Discord bot at https://discord.com/developers/applications
# 2. Copy this file to .env: cp .env.example .env
# 3. Fill in your DISCORD_BOT_TOKEN, DISCORD_GUILD_ID, DISCORD_POST_CHANNEL_ID
# 4. Pull the LLM model: docker compose run ollama ollama pull llama3.1:8b
# 5. Start services: docker compose up --build
# 6. Invite bot to your server using the OAuth2 URL from /invite command
# 7. Test with /hello, /ping, and /self_test commands
# ─────────────────────────────────────────────────────────────────────────────
